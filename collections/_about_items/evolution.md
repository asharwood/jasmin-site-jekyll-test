---
name: Evolution
layout: sectionitem_evolution
leader_text: Find out how JASMIN's infrastructure has evolved and about its roadmap for the future
leader_image: assets/img/sections/section_tiles/14EC2016_JASMIN_computing_cluster.2e16d0ba.fill-1000x500.jpg
header_background_image: assets/img/backgrounds/14EC2016_JASMIN_computing_cluste.2e16d0ba.fill-2000x1000.jpg
permalink: /about/evolution/
order: 2

phases:

 - title: Phase 1 (2011-2012)
   subtitle: A “super-data-cluster” is born
   intro_text: |
     The initial technical architecture was selected to provide a flexible, high-performance storage and data analysis environment, supporting batch computing, hosted processing and a cloud environment. The CEDA Archive had outgrown its previous hosting environment and the increasing need for scientific workdlows to “bring the compute to the data” drove the development of an infrastructure to support analysis of archive data alongside datasets brought into or generated by projects in their own collaborative workspaces.
     The first components deployed in this phase were:
       * Low latency core network
       * High-performance disk storage system supporting parallel write
       * Access to expandable tape storage for near-line storage
       * Resources to support bare-metal and virtualised compute
       * A batch scheduler
       * Block storage for storing virtual machine images
       * A paper describing the initial architecture is available (doi:10.1109/BigData.2013.6691556).
   items:
    - component: Disk Storage
      details: Initial fast disk
      extra: 4.6 PB RAL (0.5 PB Reading, 0.15 PB Leeds)
    - component: Batch compute
      details: Initial compute for LOTUS
      extra: 650 cores
    - component: Network
      details: Initial Gnodal-based network
    - component: Virtual compute
      details: VM licences
      extra: Virtualisation software licenses for hosting virtual machines
    - component: Tape storage
      details: Tape drives & media
      extra: 4 x T10KC drives, 2.5 PB media
    - component: Software
      details: "Data movement software<br>Community intercomparison suite"
    - component: Other
      details: Machine room environment monitoring equipment

 - title: Phase 1.5 (2012-2013)
   subtitle: Enabling NERC Big Data projects
   intro_text: |
     Already establishing its ability to facilitate projects with data-intensive workflows, JASMIN was given additional capability to support several NERC "Big Data" projects across a range of disciplines: near-real-time processing of EO data, Earth surface deformation analysis and seismic hazard analysis, along with supporting a cloud infrastructure used within the Genomics community.
   items:
    - component: Disk storage
      details: Minor addition to fast disk storage
      extra: 0.4 PB PFS
    - component: Batch compute
      details: Interim expansion
      extra: 1920 cores
    - component: Network
      details: Core network upgrade
    - component: Virtual compute
      details: "Virtualisation licenses: expansion of licensed estate"
    - component: Tape storage
      details: Tape drives & servers, Tape media
      extra: 2 x T10KC drives, 3.5 PB media
    - component: Software
      details: "Initial versions of Elastic Tape interface (ET)<br>& JASMIN Analysis Platform (JAP)"

 - title: Phases 2 & 3 (2013-2015)
   subtitle: Major expansion over a 2-year period
   intro_text: |
     Having proved its worth as a concept able to facilitate many large data-intensive environmental science projects, JASMIN underwent a major upgrade to provide the necessary storage and compute for its stakeholder community. Its remit now extended beyond the initial [NCAS](https://www.ncas.ac.uk) and [NCEO](https://www.nceo.ac.uk) stakeholders to serve the whole of the [NERC](https://nerc.ukri.org) community.
   items:
    - component: Disk storage
      details: "Major expansion to fast storage<br>Block storage for VM hosting<br>High-performance storage for databases"
      extra: "11 PB PFS<br>0.9 TB BLK<br>0.05 TB high-IOPS BLK"
    - component: Batch compute
      details: "Major expansion to LOTUS compute<br>Dual capability as hypervisors for virtual machines, or as LOTUS nodes"
      extra: "3800 cores<br>4 high-memory nodes (2 TB RAM)"
    - component: Network
      details: Major redesign &amp; implementation
    - component: Virtual compute
      details: Expansion of licensed estate
    - component: Tape storage
      details: Major expansion
      extra: 7.5 PB tape media
    - component: Software
      details: "Community Intercomparison Suite<br>JASMIN Cloud Portal"
      extra: "Scientific end-user software<br>Cloud tenancy management interface"
    - component: Other
      details: "User documentation<br>Website<br>Dataset construction"

 - title: Phase 3.5 (2016-2017)
   subtitle: Interim upgrades and strategic proof-of-concept projects
   intro_text: |
     Ahead of larger investments in years to come, limited but carefully-targetted upgrades ensured that key systems continued to operate at the scales needed. A proof-of-concept project tested the feasibility of using OpenStack instead of a proprietary solution for JASMIN's growing Community Cloud infrastructure.
   items:
    - component: Disk storage
      details: Object store proof of concept<br>Replacement of cloud block storage<br>Continued use of Phase 1, 2 storage inc. battery replacements
      extra: 1.2 PB HPOS<br>0.4 PB BLK
    - component: Batch compute
      details: Interim expansion of batch compute<br>Continued use of Phase 1.5 & 2 compute (~4000 cores)
      extra: 1120 cores
    - component: Network
      details: Essential network & firewall support
    - component: Virtual compute
      details: Cloud software support
    - component: Tape storage
      details: Tape media
      extra: 5 PB
    - component: Software
      details: OpenStack proof of concept

 - title: Phase 4 (2017-2018)
   subtitle: Major expansion with new technologies
   intro_text: |
     Phase 4 introduced new types of storage at the scales needed to support scientific workflows into the future. Successful proofs-of-concept with Scale Out Filesystem (SOF) and high-performance object storage (HPOS) enabled large deployments of these, with SOF adopted as the primary storage medium for Group Workspace storage, and tooling and services under development to enable use of object storage within cloud-based workflows. LOTUS gained a major upgrade of >5000 cores, in a network enhanced for future expansion. Cloud tenancies were migrated to an OpenStack platform and management interfaces adapted to match. Meanwhile testbeds for Cluster-as-a-Service and JuPyter Notebooks provided previews of exciting capabilities to come.
   items:
    - component: Disk storage
      details: BLK storage for cloud<br>Major expansion of SOF<br>Object storage (HPOS)<br>New SSD for home areas<br>Replacement of earlier PFS
      extra: 0.4 PB BLK<br>30 PB SOF<br>5 PB HPOS<br>0.5 PB SSD<br>3 PB PFS
    - component: Batch & physical compute
      details: Expansion of batch compute<br>New servers for Data Transfer Zone
      extra: 210 servers, 5040 cores<br>10 servers for DTZ
    - component: Network
      details: Implementation of "super-spine" network<br>Expansion & upgrade to management network
      extra: Ensuring future connectivity on site
    - component: Virtual compute
      details: Production deployment of OpenStack as cloud platform, migration of tenancies
    - component: Software
      details: "OpenStack upgrade for JASMIN cloud portal<br>
OpenDAP4GWS<br>Cluster-as-a-Service testbed<br>Containerised Jupyter Notebook deployed in Kubernetes"
      extra: "Management capability for OpenStack cloud tenancies<br>Autonomous exposure of data from GWSs<br>Dynamic virtualized batch compute<br>PoC for Python Notebook service"
    - component: Other
      details: "Bulk migration of data from Phase 1 hardware<br>Machine room hardware"
      extra: "Ahead of retirement of old hardware<br>Racks, PDUs, cabling, environment monitoring equipment"
    
 - title: Phase 5 (2018-2019)
   subtitle: Tape storage & other strategic upgrades
   intro_text: |
     Together with STFC's IRIS consortium, a major upgrade to a shared tape storage facility was procured with capacity for 65 PB of near-line storage. JASMIN also acquired its first GPU servers: a small prrof-of-concept cluster of 5 systems.

     It was time to say goodbye to several tonnes of storage and compute hardware from previous phases which were now retired, and needed to be removed to make room for new equipment.
   items:
    - component: Batch compute
      details: Initial GPU servers<br>Extra SSD disks for Phase 4 batch compute
      extra: PoC with 2 x small, 1 x large system
    - component: Network
      details: Firewall hardware<br>Routers and 100G connectivity
    - component: Virtual compute
      details: New hypervisor servers<br>New backup appliance
      extra: for "cattle-class" virtual machines
    - component: Tape storage
      details: Replacement of tape library<br>Tape media
      extra: Shared procurement with STFC IRIS. 65 PB capacity.<br>11 PB (LTO and TS1160)
    - component: Software
      details: OpenStack software development<br>Cluster-as-a-Service development
    - component: Other
      details: Decommissioning of Phase 2 hardware
    
 - title: Phase 6 (2019-2020)
   subtitle: Batch compute upgrade and network improvements
   intro_text: |
     LOTUS was the main focus of this phase with the replacement of old compute nodes with new higher-memory servers and work to migrate from Platform LSF to SLURM as the scheduler. A change of operating system also meant redeployment of CEDA and JASMIN service hosts throughout the system.
   items:
    - component: Disk storage
      details: BLK storage replacement
      extra: Multiple retirement dates but avoiding transition all at once.<br>To run alongside then replace existing hardware.
    - component: Batch compute
      details: Replacement of Phase 1 and 2 compute nodes
      extra: Solves flow control issue for interaction with Phase 4 storage.<br>Current 4 x 2 TB high-memory nodes to be replaced with 132 x 1 TB nodes
    - component: Network
      details: Improvements to "exit pod" network
      extra: Enhance connectivity between JASMIN & wider internet
    - component: Virtual compute
      details: Replacement of virtualisation servers
      extra: For “pet” class virtual machines where reliability is important
    - component: Software
      details: Replacement of Platform LSF with SLURM scheduler<br>Change of operating system
      extra: Move to open-source scheduler with lower ongoing costs<br>Move from RedHat Enterprise to Centos7

 - title: Phase 7 (2020-2021)
   subtitle: Essential storage upgrades and new compute capabilities 
   intro_text: |
     A much-needed boost to capacity across the many types of storage, but coupled with retirement of older disk systems and increased CPU compute for the LOTUS batch processing cluster. Following a successful proof-of-concept in previous years, this phase also establised ORCHID, JASMIN's new GPU cluster to cater for AI workflows.
   items:
    - component: Cloud
      details: Integration of an additional cloud platform 
      extra: 
    - component: Network
      details: "Replacement of Phase 1/2 network pod for Phase 7 hardware<br>25Gbit/sec NIC upgrade for hypervisors in managed cluster"
      extra:
    - component: Compute
      details: "Full-scale GPU cluster for AI workflows<br>Replacement of Phase 2/3 CPU nodes and cloud hardware expansion"
      extra: "2x8xNVidia A100 nodes, 14x4xNVidia A100 nodes<br>+768 cores CPU with large RAM.<br>New 100Gb networking for LOTUS"
    - component: Disk storage
      details: "30% SOF capacity increase, small file capability<br>40% HPOS increase<br>125% PFS capacity increase<br>SSD upgrade for small-file workloads.<br>Block capacity for virtualisation, clouds & container storage, API brought up to date"
      extra: "10 PB SOF + 0.5 PB SSD<br>2 PB HPOS<br>5 PB PFS<br>300TB SSD<br>4-500TB Flash"
    - component: Tape storage
      details: "Tape server hardware replacements<br>Tape media<br>New colder-storage system design & development to replace ET & JDMA"
      extra: "<br>18PB media"
    
 - title: Phase 7.5 / JASMINx Phase 1 (2021-2022)
   subtitle: Strategic investment in tape storage, LOTUS upgrade and consultancy on future user requirements.
   intro_text: |
     Commissioning of a new [Near-Line Data Store](https://techblog.ceda.ac.uk/2022/03/09/near-line-data-store-intro.html)(NLDS) with essential uplift in tape media capacity. Replacement and expansion of LOTUS capacity plus study of future user requirements.
   items:
    - component: Tape storage
      details: Commissioning of new NLDS tiered storage system<br>Tape media capacity increase.
      extra: NLDS design & development project underway at CEDA in collaboration with University of Reading<br>23 PB media, 4 drives, 2 data frames, chamber licences & associated costs<br>2 data servers
    - component: Compute
      details: Compute nodes to replace & expand LOTUS cluster capacity
      extra: 92 x compute nodes with 512 GB RAM, dual AMD Epyc processor, 48-core<br>Total 92 x 48 = 4416 cores, mostly for deployment in LOTUS cluster.
    - component: User requirements study
      details: Commissioned study to identify potential future user requirements for JASMIN
      extra: 


carousel:
  items:
   - image: assets/img/sections/section_content/14EC2458_JASMIN_2_computing_clus.2e16d0ba.fill-2000x1000.jpg
     caption: "All racks powered on following major addition to storage and compute capabilities in Phases 2 and 3."
   - image: assets/img/sections/section_content/20120312_DSC_3969.2e16d0ba.fill-2000x1000.jpg
     caption: "Phase 1 (2012): Panasas shelves close up."
   - image: assets/img/sections/section_content/20120308_160221.2e16d0ba.fill-2000x1000.jpg
     caption: "Phase 1 (2012) first two racks of JAMSIN storage powered on."
   - image: assets/img/sections//section_content/20120131_181541.2e16d0ba.fill-2000x1000.jpg
     caption: "Phase 1 (2012) Machine room floor before installation. Compute servers and block storage arrays."
   - image: assets/img/sections/section_content/14EC2445_JASMIN_2_computing_clus.2e16d0ba.fill-2000x1000.jpg
     caption: "Block storage added in Phase 3"
   - image: assets/img/sections/section_content/DSC7130_A7GHP9m.15015514.fill-2000x1000.jpg
     caption: "Artful cabling is required to connect across JASMIN's internal network."
  caption: JASMIN evolution in pictures.
  
     
video_src: https://www.youtube.com/embed/baCMHi1kcoM?feature=oembed
video_caption: "Spectra Time Lapse: installation of new STFC tape library"

---


Since JASMIN came into being in early 2012, it has grown significantly in scale and complexity but also in the number and variety of users it serves, and the types of scientific workflow it supports. As the requirements of its user community evolve, so does JASMIN. The Phases below describe the major procurement and upgrade projects which have taken place. These have been complemented by the work of teams within CEDA and STFC's scientific computing department in developing and maintaining the infrastructure and its component services and software to create the major e-infrastructure facility now familiar to over 1,500 users and 200 science projects.